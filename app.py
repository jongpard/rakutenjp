# -*- coding: utf-8 -*-
"""
Rakuten JP Beauty Daily Ranking (genre=100939)
- ìˆ˜ì§‘ ë²”ìœ„: 1~160ìœ„ (ì •í™• ìƒí•œ ë³´ì¥, ì´ˆê³¼ ê¸ˆì§€)
- ë Œë”ë§: Playwright (ìš°ì„ ), ì‹¤íŒ¨ ì‹œ ScraperAPI(ì˜µì…˜, í™˜ê²½ë³€ìˆ˜) ì •ì  HTML í´ë°±
- ë¡œë”© ì•ˆì •í™”: ë„¤íŠ¸ì›Œí¬ idle â†’ #rnkRankingMain ê°€ì‹œí™” â†’ ìŠ¤í¬ë¡¤ â†’ í•­ëª© ì¹´ìš´íŠ¸ ì¡°ê±´ëŒ€ê¸°
- 1~3ìœ„ ëˆ„ë½ ë°©ì§€: 1í˜ì´ì§€(1~80) ì¶”ê°€ ëŒ€ê¸°/ìŠ¤í¬ë¡¤ + 2íšŒ ì¬ì‹œë„ í•©ì§‘í•© í›„ ì¤‘ë³µì œê±°
- CSV: ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_YYYY-MM-DD.csv (KST)
- ì „ì¼ ë¹„êµ: Google Driveì—ì„œ ì „ì¼ íŒŒì¼ ë‚´ë ¤ë°›ì•„ TOP10 ìƒìŠ¹/í•˜ë½, ê¸‰í•˜ë½, ì¸&ì•„ì›ƒ ê³„ì‚°
- Slack: TOP10(ê´„í˜¸ë‚´ìš© ì œê±°), ê¸‰í•˜ë½, ì¸&ì•„ì›ƒ ê°œìˆ˜ ìš”ì•½
- í•œêµ­ì–´ ë²ˆì—­(ì˜µì…˜): SLACK_TRANSLATE_JA2KO=1 ì¼ ë•Œ ê° í•­ëª© ë°”ë¡œ ì•„ë˜ 1ì¤„ ë²ˆì—­ ì‚½ì…
- ë¸Œëœë“œ ì¶”ì •: ìƒì ëª…ì—ì„œ 'å…¬å¼|ã‚·ãƒ§ãƒƒãƒ—|ã‚¹ãƒˆã‚¢|STORE|shop' ë“± í† í° ì œê±°(ì¼ë³¸ì–´/ì˜ë¬¸ í˜¼í•©)
- í™˜ê²½ë³€ìˆ˜:
  * SLACK_WEBHOOK_URL
  * GDRIVE_FOLDER_ID (í´ë” ë§í¬/ID ëª¨ë‘ í—ˆìš©)
  * GOOGLE_CLIENT_ID / GOOGLE_CLIENT_SECRET / GOOGLE_REFRESH_TOKEN
  * RAKUTEN_GENRE_ID (ê¸°ë³¸ 100939)
  * SCRAPERAPI_KEY (ì˜µì…˜, í´ë°±ìš©)
  * RAKUTEN_MAX_RANK (ê¸°ë³¸ 160)
  * RAKUTEN_HEADLESS ("1" ê¸°ë³¸) / RAKUTEN_SLOWMO_MS (ê¸°ë³¸ 0)
  * SLACK_TRANSLATE_JA2KO ("1" ì¼œê¸°)
"""

import os, re, io, time, math, json, pytz, traceback, random
import datetime as dt
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd
from bs4 import BeautifulSoup

# ---------- ê³µí†µ/ì‹œê°„ ----------
KST = pytz.timezone("Asia/Seoul")
def now_kst(): return dt.datetime.now(KST)
def today_kst_str(): return now_kst().strftime("%Y-%m-%d")
def yesterday_kst_str(): return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
def clean_text(s): return re.sub(r"\s+", " ", (s or "")).strip()
def slack_escape(s): return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

GENRE_ID = os.getenv("RAKUTEN_GENRE_ID", "100939").strip() or "100939"
MAX_RANK  = int(os.getenv("RAKUTEN_MAX_RANK", "160"))

DAILY_URL_P1 = f"https://ranking.rakuten.co.jp/daily/{GENRE_ID}/"
DAILY_URL_P2 = f"https://ranking.rakuten.co.jp/daily/{GENRE_ID}/p=2/"

# ---------- CSV íŒŒì¼ëª… ----------
def build_filename(d): return f"ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{d}.csv"

# ---------- ìƒì ëª… â†’ ë¸Œëœë“œ ì¶”ì • ----------
OFFICIAL_TOKEN = re.compile(r"(å…¬å¼|ã‚ªãƒ•ã‚£ã‚·ãƒ£ãƒ«|OFFICIAL|official|ã‚·ãƒ§ãƒƒãƒ—|shop|Shop|SHOP|ã‚¹ãƒˆã‚¢|store|STORE|æ¥½å¤©|Rakuten|ãƒ¢ãƒ¼ãƒ«|mall)", re.I)
def infer_brand_from_shop(shop: str) -> str:
    s = clean_text(shop)
    s = OFFICIAL_TOKEN.sub("", s)
    s = re.sub(r"\s+", " ", s).strip(" -|â€¢[]()")
    return s or shop

# ---------- ê¸ˆì•¡ íŒŒì‹± ----------
YEN_RE = re.compile(r"(?:Â¥|)(\d{1,3}(?:,\d{3})+|\d+)\s*å††")
def parse_price_from_block(txt: str) -> Optional[int]:
    nums = [int(m.group(1).replace(",", "")) for m in YEN_RE.finditer(txt or "")]
    nums = [n for n in nums if n > 0]
    return min(nums) if nums else None

# ---------- ë²ˆì—­ (íí… ë¡œì§ ì´ì‹: JA ì˜ì—­ë§Œ ë²ˆì—­, ì˜µì…˜) ----------
JP_CHAR_RE = re.compile(r"[\u3040-\u30FF\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF]")
def contains_ja(s): return bool(JP_CHAR_RE.search(s or ""))

def translate_ja_to_ko_batch(lines: List[str]) -> List[str]:
    flag = os.getenv("SLACK_TRANSLATE_JA2KO", "0").lower() in ("1","true","yes")
    if not flag: return ["" for _ in lines]
    # JA ì„¸ê·¸ë¨¼íŠ¸ë§Œ ë½‘ì•„ ë°°ì¹˜ ë²ˆì—­ í›„ ì¬ì¡°ë¦½
    runs, pool = [], []
    ja_run = re.compile(r"[\u3040-\u30FF\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF]+")
    for line in lines:
        line = (line or "").strip()
        if not contains_ja(line):
            runs.append(None); continue
        parts, pos = [], 0
        for m in ja_run.finditer(line):
            if m.start() > pos: parts.append(("raw", line[pos:m.start()]))
            parts.append(("ja", line[m.start():m.end()]))
            pos = m.end()
        if pos < len(line): parts.append(("raw", line[pos:]))
        runs.append(parts)
        for k,t in parts:
            if k == "ja": pool.append(t)

    if not pool: return ["" for _ in lines]

    out_ja = []
    # 1ì°¨: googletrans (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
    try:
        from googletrans import Translator
        tr = Translator(service_urls=['translate.googleapis.com'])
        res = tr.translate(pool, src="ja", dest="ko")
        out_ja = [getattr(r,"text","") or "" for r in (res if isinstance(res,list) else [res])]
    except Exception as e:
        print("[ë²ˆì—­ ê²½ê³ ] googletrans ì‹¤íŒ¨:", e)
        try:
            from deep_translator import GoogleTranslator as DT
            gt = DT(source='ja', target='ko')
            out_ja = [gt.translate(t) if t else "" for t in pool]
        except Exception as e2:
            print("[ë²ˆì—­ ê²½ê³ ] deep-translator ì‹¤íŒ¨:", e2)
            out_ja = ["" for _ in pool]

    it = iter(out_ja)
    rebuilt = []
    for parts in runs:
        if parts is None:
            rebuilt.append("")
            continue
        buf = []
        for k,t in parts:
            buf.append(t if k=="raw" else next(it,""))
        rebuilt.append("".join(buf))
    return rebuilt

# ---------- Slack ----------
def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[INFO] Slack ë¯¸ì„¤ì • â†’ ì½˜ì†” ì¶œë ¥\n", text)
        return
    try:
        r = requests.post(url, json={"text": text}, timeout=20)
        if r.status_code >= 300:
            print("[WARN] Slack ì‹¤íŒ¨:", r.status_code, r.text)
    except Exception as e:
        print("[WARN] Slack ì˜ˆì™¸:", e)

# ---------- Google Drive ----------
def normalize_folder_id(raw: str) -> str:
    if not raw: return ""
    s = raw.strip()
    m = re.search(r"/folders/([a-zA-Z0-9_-]{10,})", s) or re.search(r"[?&]id=([a-zA-Z0-9_-]{10,})", s)
    return (m.group(1) if m else s)

def build_drive_service():
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials
    cid  = os.getenv("GOOGLE_CLIENT_ID")
    csec = os.getenv("GOOGLE_CLIENT_SECRET")
    rtk  = os.getenv("GOOGLE_REFRESH_TOKEN")
    if not (cid and csec and rtk):
        raise RuntimeError("Google OAuth ìê²©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    creds = Credentials(None, refresh_token=rtk, token_uri="https://oauth2.googleapis.com/token",
                        client_id=cid, client_secret=csec)
    svc = build("drive", "v3", credentials=creds, cache_discovery=False)
    return svc

def drive_upload_csv(service, folder_id: str, name: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)",
                               supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None
    buf = io.BytesIO(); df.to_csv(buf, index=False, encoding="utf-8-sig"); buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)
    if file_id:
        service.files().update(fileId=file_id, media_body=media,
                               supportsAllDrives=True).execute()
        return file_id
    meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
    created = service.files().create(body=meta, media_body=media, fields="id",
                                     supportsAllDrives=True).execute()
    return created["id"]

def drive_download_csv(service, folder_id: str, name: str) -> Optional[pd.DataFrame]:
    from googleapiclient.http import MediaIoBaseDownload
    res = service.files().list(q=f"name = '{name}' and '{folder_id}' in parents and trashed = false",
                               fields="files(id,name)", supportsAllDrives=True,
                               includeItemsFromAllDrives=True).execute()
    files = res.get("files", [])
    if not files: return None
    fid = files[0]["id"]
    req = service.files().get_media(fileId=fid, supportsAllDrives=True)
    fh = io.BytesIO(); dl = MediaIoBaseDownload(fh, req); done=False
    while not done: _, done = dl.next_chunk()
    fh.seek(0); return pd.read_csv(fh)

# ---------- íŒŒì„œ: DOMì—ì„œ ì•ˆì „ ì¶”ì¶œ ----------
BRACKET_PAT = re.compile(r"(\[.*?\]|ã€.*?ã€‘|ï¼ˆ.*?ï¼‰|\(.*?\))")
def strip_brackets(s: str) -> str:
    return clean_text(BRACKET_PAT.sub("", s or ""))

def _js_collect():
    # ë¸Œë¼ìš°ì € ì•ˆì—ì„œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜(ë¬¸ìì—´). ë­í‚¹ ì˜ì—­ì—ì„œ ì•„ì´í…œ ë¸”ë¡ì„ ê°•ê±´í•˜ê²Œ ìˆ˜ì§‘
    return """
() => {
  const root = document.querySelector('#rnkRankingMain');
  const out = [];
  if (!root) return out;

  // ë­í¬ ì¹´ë“œ í›„ë³´: ë§í¬ëŠ” item.rakuten.co.jp ë¡œ ì œí•œ
  const cards = root.querySelectorAll('a[href*="item.rakuten.co.jp"]');
  const seen = new Set();

  function findRank(el) {
    // ì¹´ë“œ ê·¼ì²˜ í…ìŠ¤íŠ¸ì—ì„œ "123ä½" íŒ¨í„´ ì°¾ê¸°
    let node = el;
    for (let i=0;i<6 && node;i++){
      const txt = (node.innerText||'').replace(/\\s+/g,' ').trim();
      const m = txt.match(/(\\d+)ä½/);
      if (m) return parseInt(m[1],10);
      node = node.parentElement;
    }
    return null;
  }
  function findShop(el) {
    // ìƒì ëª…: ì¹´ë“œ ê·¼ì²˜ì—ì„œ "ã‚·ãƒ§ãƒƒãƒ—"/"shop" ì˜ì—­(ì‘ì€ íšŒìƒ‰ í…ìŠ¤íŠ¸) íƒìƒ‰
    let base = el.closest('div') || el.parentElement;
    let best = '';
    if (!base) return best;
    const smalls = base.querySelectorAll('div,span,p,small');
    for (const s of smalls) {
      const t = (s.textContent||'').replace(/\\s+/g,' ').trim();
      if (!t) continue;
      if (/ã‚·ãƒ§ãƒƒãƒ—|shop|SHOP|ã‚¹ãƒˆã‚¢|store/i.test(t) || t.length<=20) {
        // í›„ë³´
        if (!best || t.length < best.length) best = t;
      }
    }
    return best;
  }

  for (const a of cards) {
    let href = a.getAttribute('href') || '';
    if (!href) continue;
    if (href.startsWith('//')) href = 'https:' + href;
    else if (href.startsWith('/')) href = 'https://ranking.rakuten.co.jp' + href;
    // ì‹¤ì œ ìƒí’ˆ ë„ë©”ì¸ìœ¼ë¡œ ë³´ì •
    if (!/https?:\\/\\/.+/.test(href)) continue;

    const name = (a.textContent||'').replace(/\\s+/g,' ').trim();
    if (!name) continue;

    const r = findRank(a);
    if (!r) continue;

    const key = r + '|' + href;
    if (seen.has(key)) continue;
    seen.add(key);

    // ê°€ê²© í…ìŠ¤íŠ¸ ê·¼ì‚¬
    const blk = (a.closest('li')||a.closest('div')||document.body).innerText.replace(/\\s+/g,' ').trim();
    const shop = findShop(a);

    out.push({rank:r, name, href, block: blk, shop});
  }
  return out;
}
"""

def render_and_collect(url: str, expect_count: int, wait_more: bool=False) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    headless = os.getenv("RAKUTEN_HEADLESS", "1") not in ("0","false","False")
    slowmo = int(os.getenv("RAKUTEN_SLOWMO_MS","0") or "0")

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=headless,
            args=["--disable-blink-features=AutomationControlled","--no-sandbox","--disable-dev-shm-usage"],
            slow_mo=slowmo
        )
        ctx = browser.new_context(
            viewport={"width": 1366, "height": 950},
            locale="ja-JP", timezone_id="Asia/Tokyo",
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"),
            extra_http_headers={"Accept-Language":"ja,en-US;q=0.9,en;q=0.8,ko;q=0.7"},
        )
        ctx.add_init_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined});")

        page = ctx.new_page()
        page.goto(url, wait_until="domcontentloaded", timeout=60_000)
        try: page.wait_for_load_state("networkidle", timeout=25_000)
        except: pass

        # ë­í‚¹ ì»¨í…Œì´ë„ˆ ëœ° ë•Œê¹Œì§€
        page.wait_for_selector("#rnkRankingMain", timeout=45_000)

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ lazy load ìê·¹
        def autoscroll(full=False):
            total = 0
            step = 800
            limit = 7000 if not full else 16000
            while total < limit:
                page.evaluate("window.scrollBy(0, %d)" % step)
                total += step
                time.sleep(0.25)

        autoscroll(full=True if wait_more else False)
        try: page.wait_for_load_state("networkidle", timeout=10_000)
        except: pass

        # í•­ëª© ìµœì†Œ ë³´ì¥ ì¡°ê±´ëŒ€ê¸°
        try:
            page.wait_for_function(
                f"() => (document.querySelectorAll('#rnkRankingMain a[href*=\"item.rakuten.co.jp\"]').length >= {max(10, expect_count//2)})",
                timeout=25_000
            )
        except: pass

        data = page.evaluate(_js_collect())
        # ë””ë²„ê·¸ HTML ì €ì¥(ì˜µì…˜)
        try:
            os.makedirs("data/debug", exist_ok=True)
            page_content = page.content()
            tag = "p1" if "p=2" not in url else "p2"
            with open(f"data/debug/rakuten_{tag}_{int(time.time())}.html","w",encoding="utf-8") as f:
                f.write(page_content)
        except: pass

        ctx.close(); browser.close()
        return data

def fetch_top160() -> List[Dict]:
    # 1í˜ì´ì§€ 2íšŒ(ì¶”ê°€ëŒ€ê¸° í¬í•¨) + 2í˜ì´ì§€ 1íšŒ â†’ í•©ì§‘í•©, ë­í¬ í‚¤ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ìš°ì„ 
    all_rows: Dict[int, Dict] = {}

    # 1~80 (ë¹ ì§ ë°©ì§€: ë³´í†µ + ì¶”ê°€ëŒ€ê¸° ë²„ì „)
    p1a = render_and_collect(DAILY_URL_P1, expect_count=60, wait_more=False)
    p1b = render_and_collect(DAILY_URL_P1, expect_count=80, wait_more=True)

    # 81~160
    p2  = render_and_collect(DAILY_URL_P2, expect_count=80, wait_more=True)

    for arr in (p1a, p1b, p2):
        for r in arr:
            rk = int(r.get("rank") or 0)
            if rk<1 or rk>MAX_RANK: continue
            all_rows[rk] = r  # ë’¤ì— ì˜¨ ë°ì´í„°ê°€ ë®ì–´ì”€(ì¶”ê°€ëŒ€ê¸°ë³¸ ìš°ì„ )

    rows = [all_rows[k] for k in sorted(all_rows.keys())]
    return rows[:MAX_RANK]

# ---------- DataFrame ë³€í™˜ ----------
def to_dataframe(items: List[Dict], date_str: str) -> pd.DataFrame:
    recs = []
    for it in items:
        price = parse_price_from_block(it.get("block",""))
        name  = clean_text(it.get("name",""))
        url   = it.get("href","")
        shop  = clean_text(it.get("shop",""))
        brand = infer_brand_from_shop(shop)

        recs.append({
            "date": date_str,
            "rank": int(it.get("rank")),
            "product_name": name,
            "price": price,
            "url": url,
            "shop": shop,
            "brand": brand,
        })
    df = pd.DataFrame(recs)
    # ì •ë ¬/í˜• ë³´ì •
    if not df.empty:
        df["rank"] = pd.to_numeric(df["rank"], errors="coerce").astype("Int64")
        df = df.drop_duplicates(subset=["rank"]).sort_values("rank").reset_index(drop=True)
    return df

# ---------- Slack ì„¹ì…˜ ë¹Œë” (íí… í¬ë§· ê¸°ë°˜) ----------
def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, List[str]]:
    S = {"top10": [], "falling": [], "inout_count": 0}

    def _plain(row):
        nm = strip_brackets(clean_text(row.get("product_name","")))
        br = clean_text(row.get("brand",""))
        if br and not nm.lower().startswith(br.lower()):
            nm = f"{br} {nm}"
        return nm

    def _link(row):
        return f"<{row['url']}|{slack_escape(_plain(row))}>"

    def _interleave(lines, jp_texts):
        kos = translate_ja_to_ko_batch(jp_texts)
        out = []
        for i, ln in enumerate(lines):
            out.append(ln)
            if kos and i < len(kos) and kos[i]:
                out.append(kos[i])
        return out

    # TOP10
    jp_rows, lines = [], []
    t10 = df_today.dropna(subset=["rank"]).sort_values("rank").head(10)
    prev_index = None
    if df_prev is not None and not df_prev.empty:
        prev_index = df_prev.set_index("url") if "url" in df_prev.columns else None

    for _, r in t10.iterrows():
        jp_rows.append(_plain(r))
        marker = ""
        if prev_index is not None and r["url"] in prev_index.index and pd.notnull(prev_index.loc[r["url"], "rank"]):
            pr, cr = int(prev_index.loc[r["url"], "rank"]), int(r["rank"])
            d = pr - cr
            marker = f"(â†‘{d}) " if d>0 else (f"(â†“{abs(d)}) " if d<0 else "")
        else:
            marker = "(New) "
        price_str = f"ï¿¥{int(r['price']):,}" if pd.notnull(r.get("price")) else "ï¿¥0"
        lines.append(f"{int(r['rank'])}. {marker}{_link(r)} â€” {price_str}")
    S["top10"] = _interleave(lines, jp_rows)

    if df_prev is None or df_prev.empty:
        return S

    # ê¸‰í•˜ë½ (Top160 ê¸°ì¤€, OUT í¬í•¨)
    t160 = df_today[(df_today["rank"].notna()) & (df_today["rank"] <= MAX_RANK)].copy()
    p160 = df_prev[(df_prev["rank"].notna()) & (df_prev["rank"] <= MAX_RANK)].copy()

    cur = t160.set_index("url"); prev = p160.set_index("url")
    common = list(set(cur.index) & set(prev.index))
    outs   = list(set(prev.index) - set(cur.index))

    movers = []
    for k in common:
        pr, cr = int(prev.loc[k,"rank"]), int(cur.loc[k,"rank"])
        drop = cr - pr
        if drop > 0:
            row = cur.loc[k]
            movers.append((drop, cr, pr, f"- {_link(row)} {pr}ìœ„ â†’ {cr}ìœ„ (â†“{drop})", _plain(row)))
    movers.sort(key=lambda x:(-x[0], x[1], x[2], x[4]))
    chosen, jp = [], []
    for _,_,_,txt,jpn in movers[:5]:
        chosen.append(txt); jp.append(jpn)

    if len(chosen) < 5:
        outs_sorted = sorted(outs, key=lambda k:int(prev.loc[k,"rank"]))
        for k in outs_sorted:
            if len(chosen) >= 5: break
            row = prev.loc[k]
            chosen.append(f"- <{k}|{slack_escape(_plain(row))}> {int(row['rank'])}ìœ„ â†’ OUT")
            jp.append(_plain(row))

    S["falling"] = _interleave(chosen, jp)

    # ì¸&ì•„ì›ƒ ê°œìˆ˜
    S["inout_count"] = len(set(cur.index) ^ set(prev.index)) // 2
    return S

def build_slack_message(date_str: str, S: Dict[str, List[str]]) -> str:
    lines = []
    lines.append(f"*Rakuten Japan ë·°í‹° ë­í‚¹ {MAX_RANK} â€” {date_str}*")
    lines.append("")
    lines.append("*TOP 10*")
    lines.extend(S.get("top10") or ["- ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append("*ğŸ“‰ ê¸‰í•˜ë½*")
    lines.extend(S.get("falling") or ["- í•´ë‹¹ ì—†ìŒ"])
    lines.append("")
    lines.append("*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{S.get('inout_count',0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)

# ---------- ì‹¤í–‰ ----------
def run_rakuten_job():
    print("[INFO] ë¼ì¿ í… ë·°í‹° ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘")
    items = []
    err = None
    for attempt in range(1, 3):  # 2íšŒ ì‹œë„
        try:
            print(f"[INFO] ë Œë” ì‹œë„ {attempt}/2")
            items = fetch_top160()
            if len(items) >= 120:  # ì•ˆì •ì„ 
                break
        except Exception as e:
            err = e
            print("[WARN] ë Œë” ì‹¤íŒ¨:", e)
            time.sleep(3)

    if not items:
        raise RuntimeError(f"ìˆ˜ì§‘ ì‹¤íŒ¨ (ì—ëŸ¬: {err})")

    # â†’ DF
    date_str = today_kst_str()
    df_today = to_dataframe(items, date_str)
    # ìƒí•œ ë³´ì¥ ë° ê²°ì¸¡ ì œê±°
    df_today = df_today.dropna(subset=["rank"]).sort_values("rank").head(MAX_RANK).reset_index(drop=True)

    print(f"[INFO] ìµœì¢… ê±´ìˆ˜: {len(df_today)} (<= {MAX_RANK})")

    # CSV ì €ì¥
    os.makedirs("data", exist_ok=True)
    file_today = build_filename(date_str)
    df_today.to_csv(os.path.join("data", file_today), index=False, encoding="utf-8-sig")
    print(f"[INFO] CSV ì €ì¥: {file_today}")

    # Drive ì—…ë¡œë“œ + ì „ì¼ ë‹¤ìš´ë¡œë“œ
    df_prev = None
    folder = normalize_folder_id(os.getenv("GDRIVE_FOLDER_ID",""))
    if folder:
        try:
            svc = build_drive_service()
            drive_upload_csv(svc, folder, file_today, df_today)
            print("[INFO] ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ OK")
            yday = yesterday_kst_str()
            file_yday = build_filename(yday)
            df_prev = drive_download_csv(svc, folder, file_yday)
            print("[INFO] ì „ì¼ CSV", "ì—†ìŒ" if df_prev is None else "í™•ì¸")
        except Exception as e:
            print("[WARN] Drive ì²˜ë¦¬ ê²½ê³ :", e)

    # Slack ë©”ì‹œì§€
    S = build_sections(df_today, df_prev)
    msg = build_slack_message(date_str, S)
    slack_post(msg)
    print("[INFO] Slack ì „ì†¡ ì™„ë£Œ")

def main():
    try:
        run_rakuten_job()
    except Exception as e:
        print("[ì˜¤ë¥˜]", e)
        traceback.print_exc()
        try:
            slack_post(f"*ë¼ì¿ í… ì¬íŒ¬ ë·°í‹° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except: pass
        raise

if __name__ == "__main__":
    main()
