# app.py â€” ë¼ì¿ í… ì¬íŒ¬ 'ë·°í‹°/ì½”ìŠ¤ë©”/í–¥ìˆ˜(100939)' ë°ì¼ë¦¬ Top160 (1~80, 81~160)
# ìš”êµ¬ì‚¬í•­:
#  - ScraperAPI(country=jp, render=true)ë¡œ ë Œë”ë§ëœ DOM ìˆ˜ì§‘
#  - ì „ì¼ ë¹„êµ = ì œí’ˆëª… ê¸°ì¤€(ì •í™• ì¼ì¹˜)
#  - ë³€ë™ì—†ìŒì€ '-' ë¡œ í‘œê¸°
#  - "â†” ë­í¬ ì¸&ì•„ì›ƒ" ì„¹ì…˜ ë¬¸êµ¬ ê³ ì •
#  - ìŠ¬ë™ í¬ë§·ì€ ê¸°ì¡´ íí… í¬ë§· ìœ ì§€

import os, io, re, sys, time, json, math, shutil, datetime as dt
import pandas as pd
import requests
from bs4 import BeautifulSoup

# -----------------------
# ê³µí†µ ì„¤ì •
# -----------------------
KST = dt.timezone(dt.timedelta(hours=9))
TODAY = dt.datetime.now(KST).date()
YMD = TODAY.strftime("%Y-%m-%d")

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DBG_DIR  = os.path.join(DATA_DIR, "debug")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(DBG_DIR, exist_ok=True)

# ì…ë ¥ URL(1~80, 81~160)
PAGE_URLS = [
    "https://ranking.rakuten.co.jp/daily/100939/",
    "https://ranking.rakuten.co.jp/daily/100939/p=2/",
]

MAX_RANK = int(os.getenv("RAKUTEN_MAX_RANK", "160"))

# Slack & Drive
SLACK_WEBHOOK_URL   = os.getenv("SLACK_WEBHOOK_URL", "")
TRANSLATE_JA2KO     = os.getenv("SLACK_TRANSLATE_JA2KO", "1") == "1"

GOOGLE_CLIENT_ID     = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
GOOGLE_REFRESH_TOKEN = os.getenv("GOOGLE_REFRESH_TOKEN")
GDRIVE_FOLDER_ID     = os.getenv("GDRIVE_FOLDER_ID")

# ScraperAPI
SCRAPERAPI_KEY = os.getenv("SCRAPERAPI_KEY", "").strip()
if not SCRAPERAPI_KEY:
    print("[ê²½ê³ ] SCRAPERAPI_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (render=false ì²´ì¸ìœ¼ë¡œë§Œ ì‹œë„)")

SESS = requests.Session()
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8,ko;q=0.7",
}

def scraperapi_get(url: str, render: bool = True, save_prefix: str = "") -> str:
    """ScraperAPIë¡œ (country=jp) HTMLì„ ê°€ì ¸ì˜¨ë‹¤. render=Trueê°€ í•µì‹¬."""
    try:
        if SCRAPERAPI_KEY:
            params = {
                "api_key": SCRAPERAPI_KEY,
                "url": url,
                "country_code": "jp",
                "render": "true" if render else "false",
                "keep_headers": "true",
                "retry_404": "true",
            }
            r = SESS.get("https://api.scraperapi.com/", params=params, headers=DEFAULT_HEADERS, timeout=60)
        else:
            # í‚¤ ì—†ìœ¼ë©´ best-effort
            r = SESS.get(url, headers=DEFAULT_HEADERS, timeout=60, allow_redirects=True)
        r.raise_for_status()
        html = r.text
        if save_prefix:
            with open(os.path.join(DBG_DIR, f"{save_prefix}.html"), "w", encoding="utf-8") as f:
                f.write(html)
        return html
    except Exception as e:
        print(f"[HTTP ì—ëŸ¬] {url} -> {e}")
        return ""

# -----------------------
# íŒŒì„œ(ë³µìˆ˜ ì…€ë ‰í„° + ë°±ì—… ì •ê·œì‹)
# -----------------------

SEL_SETS = [
    # 1) í”í•œ êµ¬ì¡°: ì¹´ë“œ ë£¨íŠ¸
    {"card": "div.rnkRanking_item"},
    # 2) ë‹¤ë¥¸ í…Œë§ˆ: li ë‹¨ìœ„
    {"card": "li[class*='rnkRanking']"},
    # 3) ë°±ì—…: data-rnk-*
    {"card": "div[id^='rnkRanking']"},  # êµ‰ì¥íˆ ëŠìŠ¨í•œ ë°±ì—…
]

def text(el):
    return re.sub(r"\s+", " ", el.get_text(strip=True)) if el else ""

def pick_one(el, selectors):
    for sel in selectors:
        f = el.select_one(sel)
        if f: return f
    return None

def parse_cards_with_css(soup: BeautifulSoup):
    items = []
    for S in SEL_SETS:
        cards = soup.select(S["card"])
        if not cards:
            continue
        for c in cards:
            # ë­í¬
            r_el = pick_one(c, [
                ".rnkRanking_rank", ".rnk_rank", ".rank", "[class*='rank']"
            ])
            # ìƒí’ˆëª…
            name_el = pick_one(c, [
                ".rnkRanking_itemName a", ".itemName a", "a.rnkRanking_itemName", "a"
            ])
            # ê°€ê²©(ìˆìœ¼ë©´)
            price_el = pick_one(c, [
                ".rnkRanking_price", ".price", "[class*='price']"
            ])
            rank = text(r_el)
            name = text(name_el)
            price = text(price_el) if price_el else ""

            # ìµœì†Œ í•„í„°
            if not rank or not name:
                continue

            # rank í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ
            m = re.search(r"\d+", rank)
            if not m:
                continue
            rank_num = int(m.group(0))

            link = ""
            if name_el and name_el.has_attr("href"):
                link = name_el["href"]
                # ìƒëŒ€ê²½ë¡œ ë³´ì •
                if link.startswith("/"):
                    link = "https://ranking.rakuten.co.jp" + link

            items.append({
                "rank": rank_num,
                "name": name,
                "price": price,
                "url": link
            })
        if items:
            break
    return items

# ë°±ì—…: ì •ê·œì‹ìœ¼ë¡œ rank & ì´ë¦„ì„ ë§¤ì¹­
RE_RANK = re.compile(r'class="[^"]*rnkRanking_rank[^"]*"[^>]*>\s*([0-9]+)\s*<', re.I)
RE_NAME = re.compile(r'class="[^"]*rnkRanking_itemName[^"]*".*?<a[^>]*>(.*?)</a>', re.I|re.S)

def parse_cards_backup_regex(html: str):
    # ì•„ì£¼ ë³´ìˆ˜ì  ë°±ì—… (ìˆœì„œëŒ€ë¡œ ëŒ€ì‘)
    ranks = [int(x) for x in RE_RANK.findall(html)]
    names = [re.sub(r"\s+", " ", re.sub("<.*?>", "", n)).strip() for n in RE_NAME.findall(html)]
    items = []
    for i, r in enumerate(ranks):
        nm = names[i] if i < len(names) else ""
        if nm:
            items.append({"rank": r, "name": nm, "price": "", "url": ""})
    return items

def fetch_one(url: str, prefix: str):
    print(f"[Playwright ëŒ€ì²´] GET(ë Œë”): {url}")
    html = scraperapi_get(url, render=True, save_prefix=prefix+"_render")
    if not html:
        return []
    soup = BeautifulSoup(html, "lxml")
    items = parse_cards_with_css(soup)
    if not items:
        # ë°±ì—…: ì •ê·œì‹
        items = parse_cards_backup_regex(html)
    print(f"[ë””ë²„ê·¸] {url} -> íŒŒì‹± {len(items)}ê±´")
    return items

# -----------------------
# ì „ì¼ CSV ëŒ€ë¹„ ë¹„êµ(ì œí’ˆëª… ê¸°ì¤€)
# -----------------------
def load_prev_csv(csv_path_today: str) -> pd.DataFrame:
    # ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ 'ì–´ì œ ë‚ ì§œ' íŒŒì¼ íƒìƒ‰
    d = os.path.dirname(csv_path_today)
    yesterday = (TODAY - dt.timedelta(days=1)).strftime("%Y-%m-%d")
    target = os.path.join(d, f"ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{yesterday}.csv")
    if os.path.exists(target):
        try:
            return pd.read_csv(target)
        except:
            pass
    return pd.DataFrame()

def build_sections(df_today: pd.DataFrame, df_prev: pd.DataFrame):
    # ë­í¬ ë³€ë™: ì˜¤ëŠ˜.rank - ì–´ì œ.rank (ì œí’ˆëª… ë§¤ì¹­)
    m = df_today[["rank","name","url","price"]].copy()
    m["prev_rank"] = None
    if not df_prev.empty:
        prev_map = {n: r for r, n in zip(df_prev["rank"], df_prev["name"])}
        m["prev_rank"] = m["name"].map(prev_map)

    def rank_delta(row):
        pr = row["prev_rank"]
        if pd.isna(pr): return None
        try:
            return int(pr) - int(row["rank"])
        except: return None

    m["delta"] = m.apply(rank_delta, axis=1)

    # ë³€ë™ í…ìŠ¤íŠ¸: â†‘/â†“/-
    def arrow(d):
        if d is None: return "-"  # ì „ì¼ ì—†ìŒë„ '-' ì²˜ë¦¬(ìš”ì²­ì‚¬í•­: ë³€ë™ì—†ìŒ í‘œê¸°)
        if d > 0: return f"â†‘{abs(d)}"
        if d < 0: return f"â†“{abs(d)}"
        return "-"

    m["delta_txt"] = m["delta"].apply(arrow)

    # Top10 í…ìŠ¤íŠ¸ (raw ì œí’ˆëª… ê·¸ëŒ€ë¡œ)
    top10 = (m.sort_values("rank").head(10))[["rank","delta_txt","name"]].values.tolist()
    # IN & OUT: ì§‘í•© ì°¨ì´
    inout = 0
    if not df_prev.empty:
        tset = set(m["name"])
        pset = set(df_prev["name"])
        ins  = tset - pset
        outs = pset - tset
        # ë„¤ê°€ ì •ì˜í•œ í…ìŠ¤íŠ¸ ê·œì¹™: "ì¸/ì•„ì›ƒ ê°œìˆ˜ëŠ” ë™ì¼" â†’ ë³´ê³ ëŠ” xê°œ
        inout = max(len(ins), len(outs))

    return m, top10, inout

# -----------------------
# Slack ì „ì†¡
# -----------------------
def slack_post(lines):
    if not SLACK_WEBHOOK_URL:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â€” ë©”ì‹œì§€ ë¯¸ì „ì†¡")
        return
    payload = {"text": "\n".join(lines)}
    try:
        r = requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=15)
        r.raise_for_status()
        print("[INFO] ìŠ¬ë™ ì „ì†¡ OK")
    except Exception as e:
        print("[ê²½ê³ ] ìŠ¬ë™ ì „ì†¡ ì‹¤íŒ¨:", e)

# -----------------------
# Google Drive ì—…ë¡œë“œ(ì„ íƒ)
# -----------------------
def upload_gdrive(local_path: str):
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        print("[INFO] ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ê±´ë„ˆëœ€(ì‹œí¬ë¦¿ ì—†ìŒ)")
        return
    try:
        from google.oauth2.credentials import Credentials
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaFileUpload

        creds = Credentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
        )
        service = build("drive", "v3", credentials=creds)
        file_metadata = {"name": os.path.basename(local_path), "parents": [GDRIVE_FOLDER_ID]}
        media = MediaFileUpload(local_path, resumable=True)
        service.files().create(body=file_metadata, media_body=media, fields="id").execute()
        print("[INFO] ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ OK:", os.path.basename(local_path))
    except Exception as e:
        print("[ê²½ê³ ] ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ì‹¤íŒ¨:", e)

# -----------------------
# ë©”ì¸
# -----------------------
def main():
    print("[INFO] ë¼ì¿ í… ë·°í‹° ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘")
    all_rows = []
    for i, url in enumerate(PAGE_URLS, start=1):
        rows = fetch_one(url, prefix=f"rakuten_p{i}")
        all_rows.extend(rows)

    # ì •ë¦¬/í•„í„°
    df = pd.DataFrame(all_rows).drop_duplicates(subset=["name"]).reset_index(drop=True)
    df = df[(df["rank"] >= 1) & (df["rank"] <= MAX_RANK)]
    df = df.sort_values("rank").reset_index(drop=True)

    print(f"[INFO] ìˆ˜ì§‘ ê°œìˆ˜: {len(df)}")

    # CSV ì €ì¥
    csv_path = os.path.join(DATA_DIR, f"ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{YMD}.csv")
    df_out = df[["rank","name","price","url"]].copy()
    df_out.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print("[INFO] ë¡œì»¬ CSV ì €ì¥:", os.path.basename(csv_path))

    # ì „ì¼ ë¹„êµ
    df_prev = load_prev_csv(csv_path)
    m, top10, inout_cnt = build_sections(df_out, df_prev)

    # ìŠ¬ë™ ë©”ì‹œì§€
    title = f"ğŸ“Š ì¼ê°„ ë¦¬í¬íŠ¸ Â· ë¼ì¿ í…JP ë·°í‹° Top160 ({YMD})"
    lines = [f"*{title}*"]
    # Top10
    lines.append("\n*ğŸ† Top10 (ì¼ê°„, raw ì œí’ˆëª…)*")
    for r, dtxt, name in top10:
        lines.append(f"{r:>3}ìœ„ | {dtxt} | {name}")

    # ì¸&ì•„ì›ƒ
    lines.append("\n*â†” ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{inout_cnt}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    slack_post(lines)

    # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ(ì˜µì…˜)
    upload_gdrive(csv_path)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e)
        raise
