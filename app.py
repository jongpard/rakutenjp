import os, time, math, json, requests
from bs4 import BeautifulSoup

BASE = "https://ranking.rakuten.co.jp/daily/100939/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
}

def fetch_page(page:int=1) -> BeautifulSoup:
    url = BASE if page == 1 else f"{BASE}p={page}/"
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def parse_items(soup: BeautifulSoup):
    items = []
    # ëª¨ë“  ë­í¬ ë¸”ë¡ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‘ ìš”ì†Œ:
    # - ìˆœìœ„: .rnkRanking_dispRank
    # - ìƒí’ˆëª… ë§í¬: .rnkRanking_itemName a
    for name_a in soup.select("div.rnkRanking_itemName a"):
        # ê°€ì¥ ê°€ê¹Œìš´ ì»¨í…Œì´ë„ˆì—ì„œ ìˆœìœ„/ê°€ê²©/ìƒµ/ë¦¬ë·°ë¥¼ ì°¾ìŒ
        container = name_a.find_parent().find_parent()  # itemName -> upperbox -> ê·¸ ìœ„
        # ì•ˆì „ì¥ì¹˜: ìƒìœ„ë¡œ ë„‰ë„‰íˆ íƒìƒ‰
        for _ in range(5):
            if container and container.select_one(".rnkRanking_dispRank"):
                break
            container = container.parent if container else None
        if not container:
            continue

        rank_tag = container.select_one(".rnkRanking_dispRank")
        price_tag = container.select_one(".rnkRanking_price")
        shop_a = container.select_one(".rnkRanking_shop a")

        rank_txt = rank_tag.get_text(strip=True) if rank_tag else ""
        # "81ä½" ì²˜ëŸ¼ ë“¤ì–´ì˜¤ë¯€ë¡œ ìˆ«ìë§Œ ì¶”ì¶œ
        rank = int("".join([c for c in rank_txt if c.isdigit()])) if rank_txt else None

        items.append({
            "rank": rank,
            "title_ja": name_a.get_text(strip=True),
            "url": name_a["href"],
            "price": price_tag.get_text(strip=True) if price_tag else "",
            "shop": shop_a.get_text(strip=True) if shop_a else "",
        })
    # ë­í¬ ê¸°ì¤€ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
    dedup = {it["rank"]: it for it in items if it["rank"] is not None}
    return [dedup[k] for k in sorted(dedup.keys())]

def collect_top(n_items=160, max_pages=13):
    results = []
    page = 1
    while len(results) < n_items and page <= max_pages:
        soup = fetch_page(page)
        page_items = parse_items(soup)
        results.extend([it for it in page_items if it["rank"] not in {x["rank"] for x in results}])
        page += 1
        time.sleep(0.5)  # ì˜ˆì˜ìƒ ì‚´ì§ ë”œë ˆì´
    # ì›í•˜ëŠ” ê°œìˆ˜ë§Œ
    results = sorted(results, key=lambda x: x["rank"])[:n_items]
    return results

# --- ë²ˆì—­ (DeepL ë˜ëŠ” Google Cloud, ì—†ìœ¼ë©´ ì›ë¬¸ ìœ ì§€) ---
import requests

def translate_ja_to_ko(texts):
    deepl_key = os.getenv("DEEPL_API_KEY")
    gcloud_key = os.getenv("GOOGLE_API_KEY")
    if deepl_key:
        url = "https://api-free.deepl.com/v2/translate"
        data = []
        for t in texts:
            data.append(("text", t))
        resp = requests.post(url, data=data + [("target_lang","KO"),("source_lang","JA")],
                             headers={"Authorization": f"DeepL-Auth-Key {deepl_key}"}, timeout=20)
        resp.raise_for_status()
        return [tr["text"] for tr in resp.json()["translations"]]
    elif gcloud_key:
        url = f"https://translation.googleapis.com/language/translate/v2?key={gcloud_key}"
        payload = {"q": texts, "source":"ja", "target":"ko", "format":"text"}
        resp = requests.post(url, json=payload, timeout=20)
        resp.raise_for_status()
        return [tr["translatedText"] for tr in resp.json()["data"]["translations"]]
    else:
        # í‚¤ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return texts

# --- ìŠ¬ë™ ì „ì†¡(ì›¹í›…) ---
def post_to_slack(items, webhook_url, title="Rakuten Japan ë·°í‹° ë­í‚¹"):
    # ìƒìœ„ 10ê°œë§Œ ë³¸ë¬¸ì— í‘œì‹œ + ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½
    top = items[:10]
    rest_count = max(0, len(items)-10)
    # ë²ˆì—­ ì¤€ë¹„
    to_translate = [f'{it["title_ja"]}' for it in top]
    ko = translate_ja_to_ko(to_translate)

    lines = [f"*{title}*"]
    for i, it in enumerate(top):
        line = f'{it["rank"]}. {it["title_ja"]}\n   â–¶ {ko[i]}\n   ğŸ’´ {it["price"]} | ğŸ¬ {it["shop"]} | <{it["url"]}|ìƒí’ˆë§í¬>'
        lines.append(line)
    if rest_count:
        lines.append(f"â€¦ ê·¸ë¦¬ê³  {rest_count}ê°œ í•­ëª© ë” ìˆ˜ì§‘ë¨.")

    payload = {"text": "\n".join(lines)}
    r = requests.post(webhook_url, json=payload, timeout=15)
    r.raise_for_status()

if __name__ == "__main__":
    # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘ (ì˜ˆ: 1~160ìœ„)
    items = collect_top(n_items=160)
    # CSV ì €ì¥ ì˜ˆì‹œ
    import csv, datetime
    ts = datetime.datetime.now().strftime("%Y-%m-%d")
    fname = f"rakuten_beauty_{ts}.csv"
    with open(fname, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=["rank","title_ja","price","shop","url"])
        w.writeheader()
        w.writerows(items)

    # ìŠ¬ë™ ì „ì†¡ (í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URL ì‚¬ìš©)
    webhook = os.getenv("SLACK_WEBHOOK_URL")
    if webhook:
        post_to_slack(items, webhook_url=webhook, title=f"Rakuten Japan ë·°í‹° ë­í‚¹ {ts}")
    print(f"[INFO] ìˆ˜ì§‘ ì™„ë£Œ: {len(items)}ê°œ, íŒŒì¼: {fname}")
