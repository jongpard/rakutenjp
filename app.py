# -*- coding: utf-8 -*-
"""
Rakuten JP Beauty(100939) Daily Rank 1~160
- ScraperAPI(JP)ë¡œ ìš°íšŒ, render=False â†’ 0ê°œë©´ True ì¬ì‹œë„(í¬ë ˆë”§ ì ˆì•½)
- íŒŒì‹±: div.rnkRanking_after4box ê¸°ì¤€ (rank/name/url/price/shop/brand)
- CSV ì €ì¥: ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_YYYY-MM-DD.csv
- (ì˜µì…˜) Google Drive ì—…ë¡œë“œ + ì „ì¼ ë¹„êµë¡œ Slack ë©”ì‹œì§€ ì „ì†¡
- ë³€ë™ì´ ì—†ìœ¼ë©´ "-" ë¡œ í‘œê¸°

í•„ìˆ˜ env:
  SCRAPERAPI_KEY, SLACK_WEBHOOK_URL,
  GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_REFRESH_TOKEN, GDRIVE_FOLDER_ID
ì˜µì…˜ env:
  RAKUTEN_MAX_RANK(ê¸°ë³¸ 160), RAKUTEN_FORCE_RENDER(ê¸°ë³¸ 0), RAKUTEN_SAVE_DEBUG(ê¸°ë³¸ 1)
"""

import os, re, io, time, traceback, datetime as dt
from typing import List, Dict, Optional

import requests
import pandas as pd
from bs4 import BeautifulSoup

# ===== ê¸°ë³¸ =====
KST = dt.timezone(dt.timedelta(hours=9))
def now_kst(): return dt.datetime.now(KST)
def today(): return now_kst().strftime("%Y-%m-%d")
def yesterday(): return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
def build_filename(d): return f"ë¼ì¿ í…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_{d}.csv"
def clean(s: str) -> str: return re.sub(r"\s+", " ", (s or "")).strip()
def slack_escape(s: str) -> str: return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

DATA_DIR = "data"; DBG_DIR = "data/debug"
os.makedirs(DATA_DIR, exist_ok=True); os.makedirs(DBG_DIR, exist_ok=True)

MAX_RANK = int(os.getenv("RAKUTEN_MAX_RANK", "160"))
FORCE_RENDER = os.getenv("RAKUTEN_FORCE_RENDER", "0") in ("1","true","True")
SAVE_DEBUG   = os.getenv("RAKUTEN_SAVE_DEBUG", "1") in ("1","true","True")

# ëŒ€ìƒ í˜ì´ì§€(1~80, 81~160)
RANK_URLS = [
    "https://ranking.rakuten.co.jp/daily/100939/",
    "https://ranking.rakuten.co.jp/daily/100939/p=2/",
]

# ===== ScraperAPI =====
SCRAPER_KEY = os.getenv("SCRAPERAPI_KEY", "").strip()
SCRAPER_ENDPOINT = "https://api.scraperapi.com/"
HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/120.0.0.0 Safari/537.36"),
    "Accept-Language": "ja,en-US;q=0.9"
}

def scraperapi_get(url: str, render: bool) -> str:
    if not SCRAPER_KEY:
        raise RuntimeError("SCRAPERAPI_KEY ë¯¸ì„¤ì •")
    params = {
        "api_key": SCRAPER_KEY,
        "url": url,
        "country_code": "jp",
        "retry_404": "true",
        "keep_headers": "true",
        "render": "true" if render else "false",
    }
    r = requests.get(SCRAPER_ENDPOINT, params=params, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return r.text

# ===== íŒŒì‹± =====
YEN_RE = re.compile(r"([0-9,]+)\s*å††")

BRAND_STOPWORDS = [
    "æ¥½å¤©å¸‚å ´åº—","å…¬å¼","ã‚ªãƒ•ã‚£ã‚·ãƒ£ãƒ«","ã‚·ãƒ§ãƒƒãƒ—","ã‚¹ãƒˆã‚¢","å°‚é–€åº—","ç›´å–¶",
    "åº—","æœ¬åº—","æ”¯åº—","æ¥½å¤©å¸‚å ´","æ¥½å¤©","mall","MALL","shop","SHOP","store","STORE"
]
def brand_from_shop(shop: str) -> str:
    b = clean(shop)
    for w in BRAND_STOPWORDS:
        b = re.sub(w, "", b, flags=re.IGNORECASE)
    b = re.sub(r"[ã€ã€‘\[\]ï¼ˆï¼‰()]", "", b)
    return b.strip(" -_Â·|Â·")

def parse_rank_page(html: str, add_offset: int) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    cards = soup.select("div.rnkRanking_after4box")
    rows: List[Dict] = []
    for c in cards:
        r_el = c.select_one(".rnkRanking_dispRank")
        if not r_el: 
            continue
        rk_txt = clean(r_el.get_text())
        rk_m = re.search(r"\d+", rk_txt)
        if not rk_m: 
            continue
        rank = int(rk_m.group(0))

        a = c.select_one(".rnkRanking_itemName a")
        name = clean(a.get_text()) if a else ""
        href = a.get("href") if a else ""
        if href: href = re.sub(r"[?#].*$","",href.strip())

        pr_el = c.select_one(".rnkRanking_price")
        pr_txt = clean(pr_el.get_text()) if pr_el else ""
        m = YEN_RE.search(pr_txt or "")
        price = int(m.group(1).replace(",", "")) if m else None

        sh_a = c.select_one(".rnkRanking_shop a")
        shop = clean(sh_a.get_text()) if sh_a else ""
        brand = brand_from_shop(shop)

        rows.append({"rank": rank + add_offset*0, "product_name": name,
                     "price": price, "url": href, "shop": shop, "brand": brand})
    return rows

def fetch_all() -> List[Dict]:
    allrows: List[Dict] = []
    for url in RANK_URLS:
        add = 80 if "p=2" in url else 0
        # 1ì°¨: render=False(ì ˆì•½) or ê°•ì œ ì„¤ì •
        render_first = True if FORCE_RENDER else False
        html = scraperapi_get(url, render=render_first)
        if SAVE_DEBUG:
            open(f"{DBG_DIR}/rakuten_{'p2' if add else 'p1'}_raw_{'r1' if render_first else 'r0'}.html","w",encoding="utf-8").write(html)
        rows = parse_rank_page(html, add)
        if len(rows) == 0 and not render_first:
            # 2ì°¨: í•´ë‹¹ í˜ì´ì§€ë§Œ ë Œë” ON ì¬ì‹œë„
            html = scraperapi_get(url, render=True)
            if SAVE_DEBUG:
                open(f"{DBG_DIR}/rakuten_{'p2' if add else 'p1'}_raw_r1.html","w",encoding="utf-8").write(html)
            rows = parse_rank_page(html, add)
        allrows.extend(rows)
        time.sleep(0.6)
    allrows = sorted(allrows, key=lambda r: r["rank"])[:MAX_RANK]
    return allrows

# ===== Slack =====
def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[Slack ë¯¸ì„¤ì •] ë©”ì‹œì§€ ìƒëµ")
        return
    try:
        r = requests.post(url, json={"text": text}, timeout=25)
        if r.status_code >= 300:
            print("[Slack ì‹¤íŒ¨]", r.status_code, r.text[:300])
    except Exception as e:
        print("[Slack ì˜ˆì™¸]", e)

def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, list]:
    S = {"top10": [], "falling": [], "inout_count": 0}
    if "rank" not in df_today.columns or len(df_today) == 0:
        return S

    def _name(r):
        nm = clean(r.get("product_name",""))
        br = clean(r.get("brand",""))
        return f"{br} {nm}" if br and not nm.lower().startswith(br.lower()) else nm

    def _link(r):
        return f"<{r['url']}|{slack_escape(_name(r))}>" if r.get("url") else slack_escape(_name(r))

    prev_idx = None
    if df_prev is not None and len(df_prev) and "rank" in df_prev.columns:
        prev_idx = df_prev.copy()
        prev_idx["__k__"] = prev_idx["product_name"].astype(str).str.strip()
        prev_idx.set_index("__k__", inplace=True)

    # TOP10
    top10 = df_today.dropna(subset=["rank"]).sort_values("rank").head(10)
    lines = []
    for _, r in top10.iterrows():
        mark = ""
        if prev_idx is not None:
            k = str(r.get("product_name")).strip()
            if k in prev_idx.index and pd.notnull(prev_idx.loc[k, "rank"]):
                pr = int(prev_idx.loc[k, "rank"]); cr = int(r["rank"])
                diff = pr - cr
                if diff > 0: mark = f"(â†‘{diff}) "
                elif diff < 0: mark = f"(â†“{abs(diff)}) "
                else: mark = "(-) "
            else:
                mark = "(New) "
        price_txt = f"ï¿¥{int(r['price']):,}" if pd.notnull(r.get("price")) else "ï¿¥0"
        lines.append(f"{int(r['rank'])}. {mark}{_link(r)} â€” {price_txt}")
    S["top10"] = lines

    if prev_idx is None:
        return S

    cur_idx = df_today.copy()
    cur_idx["__k__"] = cur_idx["product_name"].astype(str).str.strip()
    cur_idx.set_index("__k__", inplace=True)

    tN = cur_idx[(cur_idx["rank"].notna()) & (cur_idx["rank"] <= MAX_RANK)]
    pN = prev_idx[(prev_idx["rank"].notna()) & (prev_idx["rank"] <= MAX_RANK)]

    common = set(tN.index) & set(pN.index)
    out_only = set(pN.index) - set(tN.index)

    movers = []
    for k in common:
        pr, cr = int(pN.loc[k, "rank"]), int(tN.loc[k, "rank"])
        drop = cr - pr
        if drop > 0:
            row = tN.loc[k]
            movers.append((drop, cr, pr, f"- {_link(row)} {pr}ìœ„ â†’ {cr}ìœ„ (â†“{drop})"))
    movers.sort(key=lambda x: (-x[0], x[1], x[2]))
    S["falling"] = [m[3] for m in movers[:5]]

    if len(S["falling"]) < 5:
        outs = sorted(list(out_only), key=lambda k: int(pN.loc[k, "rank"]))
        for k in outs:
            if len(S["falling"]) >= 5: break
            row = pN.loc[k]
            S["falling"].append(f"- {slack_escape(str(k))} {int(row['rank'])}ìœ„ â†’ OUT")

    today_keys, prev_keys = set(tN.index), set(pN.index)
    S["inout_count"] = len(today_keys ^ prev_keys) // 2
    return S

def build_slack_message(date_str: str, S: Dict[str, list]) -> str:
    lines = []
    lines.append(f"*Rakuten Japan ë·°í‹° ë­í‚¹ {MAX_RANK} â€” {date_str}*")
    lines.append("")
    if S["top10"]:
        lines.append("*TOP 10*"); lines.extend(S["top10"])
        lines.append(""); lines.append("*ğŸ“‰ ê¸‰í•˜ë½*"); lines.extend(S.get("falling") or ["- í•´ë‹¹ ì—†ìŒ"])
        lines.append(""); lines.append("*â†” ë­í¬ ì¸&ì•„ì›ƒ*")
        lines.append(f"{S.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        lines.append("_ìˆ˜ì§‘ëœ ë­í‚¹ì´ ì—†ìŠµë‹ˆë‹¤. data/debug HTMLì„ í™•ì¸í•˜ì„¸ìš”._")
    return "\n".join(lines)

# ===== Google Drive =====
def normalize_folder_id(raw: str) -> str:
    if not raw: return ""
    s = raw.strip()
    m = re.search(r"/folders/([a-zA-Z0-9_-]{10,})", s) or re.search(r"[?&]id=([a-zA-Z0-9_-]{10,})", s)
    return (m.group(1) if m else s)

def build_drive_service():
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials
    cid, csec, rtk = (os.getenv("GOOGLE_CLIENT_ID"), os.getenv("GOOGLE_CLIENT_SECRET"), os.getenv("GOOGLE_REFRESH_TOKEN"))
    creds = Credentials(None, refresh_token=rtk, token_uri="https://oauth2.googleapis.com/token",
                        client_id=cid, client_secret=csec)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def drive_upload_csv(service, folder_id: str, name: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)", supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None
    buf = io.BytesIO(); df.to_csv(buf, index=False, encoding="utf-8-sig"); buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)
    if file_id:
        service.files().update(fileId=file_id, media_body=media, supportsAllDrives=True).execute(); return file_id
    meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
    created = service.files().create(body=meta, media_body=media, fields="id", supportsAllDrives=True).execute()
    return created["id"]

def drive_download_csv(service, folder_id: str, name: str) -> Optional[pd.DataFrame]:
    from googleapiclient.http import MediaIoBaseDownload
    res = service.files().list(q=f"name = '{name}' and '{folder_id}' in parents and trashed = false",
                               fields="files(id,name)", supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
    files = res.get("files", [])
    if not files: return None
    fid = files[0]["id"]
    req = service.files().get_media(fileId=fid, supportsAllDrives=True)
    fh = io.BytesIO(); dl = MediaIoBaseDownload(fh, req); done = False
    while not done: _, done = dl.next_chunk()
    fh.seek(0); return pd.read_csv(fh)

# ===== ë©”ì¸ =====
def main():
    print("[INFO] ë¼ì¿ í… ë·°í‹° ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘(ScraperAPI, ì ˆì•½ëª¨ë“œ)")
    rows = fetch_all()
    print("[INFO] ìˆ˜ì§‘:", len(rows))

    date_str = today()
    df_today = pd.DataFrame(rows)
    df_today.insert(0, "date", date_str)

    # CSV ì €ì¥
    os.makedirs(DATA_DIR, exist_ok=True)
    file_today = build_filename(date_str)
    df_today[["rank","product_name","price","url","shop","brand"]].to_csv(
        os.path.join(DATA_DIR, file_today), index=False, encoding="utf-8-sig"
    )
    print("[INFO] ë¡œì»¬ CSV ì €ì¥:", file_today)

    # Drive ì—…ë¡œë“œ + ì „ì¼ ë¡œë“œ
    df_prev = None
    folder = normalize_folder_id(os.getenv("GDRIVE_FOLDER_ID", ""))
    if folder:
        try:
            svc = build_drive_service()
            drive_upload_csv(svc, folder, file_today, df_today)
            y_name = build_filename(yesterday())
            df_prev = drive_download_csv(svc, folder, y_name)
            print("[INFO] ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ OK, ì „ì¼:", "ìˆìŒ" if df_prev is not None else "ì—†ìŒ")
        except Exception as e:
            print("[Drive ì˜¤ë¥˜]", e); traceback.print_exc()
    else:
        print("[INFO] GDRIVE_FOLDER_ID ë¯¸ì„¤ì • â†’ ì—…ë¡œë“œ ìƒëµ")

    # Slack
    S = build_sections(df_today, df_prev)
    slack_post(build_slack_message(date_str, S))
    print("[INFO] Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ì˜¤ë¥˜]", e); traceback.print_exc()
        try: slack_post(f"*ë¼ì¿ í… ë­í‚¹ ì‹¤íŒ¨*\n```\n{e}\n```")
        except: pass
        raise
